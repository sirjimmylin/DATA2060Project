{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### What is Naive Bayes?\n",
        "\n",
        "Naive Bayes is a **probabilistic classifier** based on Bayes’ theorem:\n",
        "\n",
        "$$\n",
        "P(Y \\mid X) = \\frac{P(X \\mid Y)\\, P(Y)}{P(X)}\n",
        "$$\n",
        "\n",
        "It makes the **conditional independence assumption**, meaning that given the class \\(Y = c\\), all features $(x_1, ..., x_d)$ are independent:\n",
        "\n",
        "$$\n",
        "P(X \\mid Y=c) = \\prod_{j=1}^{d} P(x_j \\mid Y=c)\n",
        "$$\n"
      ],
      "metadata": {
        "id": "FUdLmd8xOzZv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why \"Gaussian\"?\n",
        "\n",
        "For continuous features, Gaussian Naive Bayes assumes each feature follows a **normal (Gaussian) distribution** under each class:\n",
        "\n",
        "$$\n",
        "x_j \\mid Y = c \\sim \\mathcal{N}(\\mu_{jc},\\, \\sigma_{jc}^2)\n",
        "$$\n"
      ],
      "metadata": {
        "id": "8V-JVqebOzWr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Steps of the Algorithm\n",
        "\n",
        "**1. Estimate priors:**\n",
        "\n",
        "The prior probability of each class \\(c\\) is:\n",
        "\n",
        "$$\n",
        "P(Y = c) = \\frac{\\text{count of class } c}{N}\n",
        "$$\n",
        "\n",
        "**2. Estimate mean and variance** for every feature \\(j\\) and every class \\(c\\).\n",
        "\n",
        "**3. Prediction:**  \n",
        "Compute the **posterior probability** for each class:\n",
        "\n",
        "$$\n",
        "P(Y=c \\mid X) \\propto P(Y=c) \\prod_{j=1}^{d} P(x_j \\mid Y=c)\n",
        "$$\n",
        "\n",
        "Then **choose the class with the maximum posterior probability**.\n"
      ],
      "metadata": {
        "id": "Mp3NOTDEOzTk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advantages\n",
        "\n",
        "- Extremely fast training  \n",
        "- Closed-form parameter estimates  \n",
        "- Works well with small datasets  \n",
        "- No gradient descent required  \n",
        "- Robust to irrelevant features  \n",
        "- Strong baseline classifier  \n"
      ],
      "metadata": {
        "id": "kpdWHHCTOzQe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Disadvantages\n",
        "\n",
        "- Assumes independence — rarely true in real data  \n",
        "- Assumes features follow a normal distribution — often incorrect  \n",
        "- Performs poorly when features are highly correlated  \n",
        "- Decision boundaries are quadratic, which may limit flexibility  \n"
      ],
      "metadata": {
        "id": "0SsLH1KnOzM2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Representation\n",
        "\n",
        "Naive Bayes converts input features into a class prediction by computing the **posterior probability** for each class:\n",
        "\n",
        "$$\n",
        "P(Y = c \\mid X = x_1, \\ldots, x_d) \\propto\n",
        "P(Y = c)\\, \\prod_{j=1}^{d} \\mathcal{N}(x_j \\mid \\mu_{jc},\\, \\sigma_{jc}^2)\n",
        "$$\n",
        "\n",
        "The classifier predicts the class with the highest posterior.  \n",
        "In practice, we work with **log probabilities** (to avoid numerical underflow):\n",
        "\n",
        "$$\n",
        "\\hat{y} =\n",
        "\\arg\\max_{c}\n",
        "\\left[\n",
        "\\log P(Y = c)\n",
        "+\n",
        "\\sum_{j=1}^{d} \\log \\mathcal{N}(x_j \\,;\\, \\mu_{jc},\\, \\sigma_{jc}^2)\n",
        "\\right]\n",
        "$$\n",
        "\n",
        "We use **log probabilities for numerical stability** because multiplying many small Gaussian likelihoods can lead to extremely tiny numbers that computers cannot represent reliably. Summing logs avoids this problem.\n"
      ],
      "metadata": {
        "id": "H33etHloOzJa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss Function\n",
        "\n",
        "**Important:** Naive Bayes does **not** minimize a traditional loss like MSE or cross-entropy using gradient descent.\n",
        "\n",
        "Instead, the model is trained by **maximizing the likelihood** of the data.  \n",
        "Equivalently, the loss is the **negative log-likelihood**:\n",
        "\n",
        "$$\n",
        "L = -\\sum_{i=1}^{N} \\log P\\big(y^{(i)} \\mid x^{(i)}\\big)\n",
        "$$\n",
        "\n",
        "Gaussian Naive Bayes maximizes the likelihood under the assumption that each feature is normally distributed for each class.\n",
        "\n",
        "The parameters come directly from **Maximum Likelihood Estimation (MLE)**:\n",
        "\n",
        "**Mean estimate:**\n",
        "\n",
        "$$\n",
        "\\mu_{jc} = \\frac{1}{N_c} \\sum_{i : y_i = c} x_{ij}\n",
        "$$\n",
        "\n",
        "**Variance estimate:**\n",
        "\n",
        "$$\n",
        "\\sigma_{jc}^2 = \\frac{1}{N_c} \\sum_{i : y_i = c} (x_{ij} - \\mu_{jc})^2\n",
        "$$\n",
        "\n",
        "Because the parameters have **closed-form MLE solutions**,  \n",
        "No gradient descent, No iterative optimization is required during training.\n"
      ],
      "metadata": {
        "id": "VBvGM_uLOzGJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizer\n",
        "\n",
        "Naive Bayes does **not** use an iterative optimizer such as gradient descent.  \n",
        "Gaussian Naive Bayes has **closed-form Maximum Likelihood Estimates (MLE)** for all parameters.\n",
        "\n",
        "**Prior probabilities:**\n",
        "\n",
        "$$\n",
        "\\hat{P}(Y = c) = \\frac{N_c}{N}\n",
        "$$\n",
        "\n",
        "**Gaussian parameters** for each feature \\(j\\) and class \\(c\\):\n",
        "\n",
        "- Mean: $\\mu_{jc}$  \n",
        "- Variance: $\\sigma_{jc}^2$  \n",
        "\n",
        "These are computed **directly from the data** using simple MLE formulas.  \n",
        "No optimization loop is required.\n"
      ],
      "metadata": {
        "id": "mOItq5lYOzDL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Citations\n",
        "\n",
        "- Collins, M. (2002). The Naive Bayes model, maximum‑likelihood estimation, and the EM algorithm (Technical Report). Columbia University.\n",
        "- Rish, I. (2001). An empirical study of the naive Bayes classifier. IBM T.J. Watson Research Center.\n",
        "- Friedman, N., Geiger, D., & Goldszmidt, M. (1997). Bayesian Network Classifiers. Machine Learning, 29(2–3), 131–163.\n",
        "- Zaidi, N. A., Cerquides, J., Carman, M. J., & Webb, G. I. (2013). Alleviating Naive Bayes attribute independence assumption by attribute weighting. Journal of Machine Learning Research, 14, 1947‑1988.\n",
        "- Ahmed, M. S., Shahjaman, M., Rana, M. M., & Mollah, M. N. H. (2017). Robustification of Naïve Bayes Classifier and Its Application for Microarray Gene Expression Data Analysis. BioMed research international, 2017, 3020627.\n",
        "- John, G. H., & Langley, P. (2013). Estimating continuous distributions in Bayesian classifiers [Preprint]. arXiv.\n"
      ],
      "metadata": {
        "id": "A_8VZv7zWc8B"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jdvH5Pg0dx5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overview of [the name of your ML algorithm] (20 points)\n",
        "\n",
        "Give an overview of the algorithm and describe its advantages and disadvantages.\n",
        "\n",
        "Representation: describe how the feature values are converted into a single number prediction.\n",
        "\n",
        "Loss: describe the metric used to measure the difference between the model’s prediction and the target variable.\n",
        "\n",
        "Optimizer: describe the numerical algorithm used to find the model parameters that minimize the loss given a training set.\n",
        "\n",
        "Use markdown in the jupyter notebook, add equations to explain math, and use pseudo-code to explain how numerical algorithms work. Use citations and references. Use at least 500 words (excluding equations and pseudo-code).\n"
      ],
      "metadata": {
        "id": "8vAEYegXNLey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ta email - I've seen that you already completed the first step which is to choose what ML algo you want to work on, that's great!\n",
        "As you can read in the final project rubric, the next step is to complete the markdown section of the report and make sure everyone understands the math and numerical methods behind the algorithm.\n",
        "\n",
        "It would be great if you could finish this by the end of this current week, in order to schedule a meeting next week to validate that you are on the right track!"
      ],
      "metadata": {
        "id": "ibi6M_YCN_l7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gaussian Naive Bayes for classification\n",
        "We cover the Naive Bayes algorithm for categorical (binary) features (Chapter 24.0 and 24.1 in the textbook). Gaussian Naive Bayes is an extension of the method to continuous features. You can read more about this algorithm here.\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB\n",
        "\n",
        "\n",
        "\n",
        "https://www-cambridge-org.revproxy.brown.edu/core/services/aop-cambridge-core/content/view/ABD3A52A2171432702023317201AC255/9781107298019c24_p295-308_CBO.pdf/generative_models.pdf"
      ],
      "metadata": {
        "id": "bKTxhyK0LSuq"
      }
    }
  ]
}