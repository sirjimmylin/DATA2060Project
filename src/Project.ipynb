{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUdLmd8xOzZv"
      },
      "source": [
        "### What is Naive Bayes?\n",
        "\n",
        "Naive Bayes is a **probabilistic classifier** based on Bayes’ theorem:\n",
        "\n",
        "$$\n",
        "P(Y \\mid X) = \\frac{P(X \\mid Y)\\, P(Y)}{P(X)}\n",
        "$$\n",
        "\n",
        "It makes the **conditional independence assumption**, meaning that given the class \\(Y = c\\), all features $(x_1, ..., x_d)$ are independent:\n",
        "\n",
        "$$\n",
        "P(X \\mid Y=c) = \\prod_{j=1}^{d} P(x_j \\mid Y=c)\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8V-JVqebOzWr"
      },
      "source": [
        "### Why \"Gaussian\"?\n",
        "\n",
        "For continuous features, Gaussian Naive Bayes assumes each feature follows a **normal (Gaussian) distribution** under each class:\n",
        "\n",
        "$$\n",
        "x_j \\mid Y = c \\sim \\mathcal{N}(\\mu_{jc},\\, \\sigma_{jc}^2)\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp3NOTDEOzTk"
      },
      "source": [
        "### Steps of the Algorithm\n",
        "\n",
        "**1. Estimate priors:**\n",
        "\n",
        "The prior probability of each class \\(c\\) is:\n",
        "\n",
        "$$\n",
        "P(Y = c) = \\frac{\\text{count of class } c}{N}\n",
        "$$\n",
        "\n",
        "**2. Estimate mean and variance** for every feature \\(j\\) and every class \\(c\\).\n",
        "\n",
        "**3. Prediction:**  \n",
        "Compute the **posterior probability** for each class:\n",
        "\n",
        "$$\n",
        "P(Y=c \\mid X) \\propto P(Y=c) \\prod_{j=1}^{d} P(x_j \\mid Y=c)\n",
        "$$\n",
        "\n",
        "Then **choose the class with the maximum posterior probability**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpdWHHCTOzQe"
      },
      "source": [
        "### Advantages\n",
        "\n",
        "- Extremely fast training  \n",
        "- Closed-form parameter estimates  \n",
        "- Works well with small datasets  \n",
        "- No gradient descent required  \n",
        "- Robust to irrelevant features  \n",
        "- Strong baseline classifier  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SsLH1KnOzM2"
      },
      "source": [
        "### Disadvantages\n",
        "\n",
        "- Assumes independence — rarely true in real data  \n",
        "- Assumes features follow a normal distribution — often incorrect  \n",
        "- Performs poorly when features are highly correlated  \n",
        "- Decision boundaries are quadratic, which may limit flexibility  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H33etHloOzJa"
      },
      "source": [
        "### Representation\n",
        "\n",
        "Naive Bayes converts input features into a class prediction by computing the **posterior probability** for each class:\n",
        "\n",
        "$$\n",
        "P(Y = c \\mid X = x_1, \\ldots, x_d) \\propto\n",
        "P(Y = c)\\, \\prod_{j=1}^{d} \\mathcal{N}(x_j \\mid \\mu_{jc},\\, \\sigma_{jc}^2)\n",
        "$$\n",
        "\n",
        "The classifier predicts the class with the highest posterior.  \n",
        "In practice, we work with **log probabilities** (to avoid numerical underflow):\n",
        "\n",
        "$$\n",
        "\\hat{y} =\n",
        "\\arg\\max_{c}\n",
        "\\left[\n",
        "\\log P(Y = c)\n",
        "+\n",
        "\\sum_{j=1}^{d} \\log \\mathcal{N}(x_j \\,;\\, \\mu_{jc},\\, \\sigma_{jc}^2)\n",
        "\\right]\n",
        "$$\n",
        "\n",
        "We use **log probabilities for numerical stability** because multiplying many small Gaussian likelihoods can lead to extremely tiny numbers that computers cannot represent reliably. Summing logs avoids this problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBvGM_uLOzGJ"
      },
      "source": [
        "### Loss Function\n",
        "\n",
        "**Important:** Naive Bayes does **not** minimize a traditional loss like MSE or cross-entropy using gradient descent.\n",
        "\n",
        "Instead, the model is trained by **maximizing the likelihood** of the data.  \n",
        "Equivalently, the loss is the **negative log-likelihood**:\n",
        "\n",
        "$$\n",
        "L = -\\sum_{i=1}^{N} \\log P\\big(y^{(i)} \\mid x^{(i)}\\big)\n",
        "$$\n",
        "\n",
        "Gaussian Naive Bayes maximizes the likelihood under the assumption that each feature is normally distributed for each class.\n",
        "\n",
        "The parameters come directly from **Maximum Likelihood Estimation (MLE)**:\n",
        "\n",
        "**Mean estimate:**\n",
        "\n",
        "$$\n",
        "\\mu_{jc} = \\frac{1}{N_c} \\sum_{i : y_i = c} x_{ij}\n",
        "$$\n",
        "\n",
        "**Variance estimate:**\n",
        "\n",
        "$$\n",
        "\\sigma_{jc}^2 = \\frac{1}{N_c} \\sum_{i : y_i = c} (x_{ij} - \\mu_{jc})^2\n",
        "$$\n",
        "\n",
        "Because the parameters have **closed-form MLE solutions**,  \n",
        "No gradient descent, No iterative optimization is required during training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOItq5lYOzDL"
      },
      "source": [
        "### Optimizer\n",
        "\n",
        "Naive Bayes does **not** use an iterative optimizer such as gradient descent.  \n",
        "Gaussian Naive Bayes has **closed-form Maximum Likelihood Estimates (MLE)** for all parameters.\n",
        "\n",
        "**Prior probabilities:**\n",
        "\n",
        "$$\n",
        "\\hat{P}(Y = c) = \\frac{N_c}{N}\n",
        "$$\n",
        "\n",
        "**Gaussian parameters** for each feature \\(j\\) and class \\(c\\):\n",
        "\n",
        "- Mean: $\\mu_{jc}$  \n",
        "- Variance: $\\sigma_{jc}^2$  \n",
        "\n",
        "These are computed **directly from the data** using simple MLE formulas.  \n",
        "No optimization loop is required.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_8VZv7zWc8B"
      },
      "source": [
        "### Citations\n",
        "\n",
        "- Collins, M. (2002). The Naive Bayes model, maximum‑likelihood estimation, and the EM algorithm (Technical Report). Columbia University.\n",
        "- Rish, I. (2001). An empirical study of the naive Bayes classifier. IBM T.J. Watson Research Center.\n",
        "- Friedman, N., Geiger, D., & Goldszmidt, M. (1997). Bayesian Network Classifiers. Machine Learning, 29(2–3), 131–163.\n",
        "- Zaidi, N. A., Cerquides, J., Carman, M. J., & Webb, G. I. (2013). Alleviating Naive Bayes attribute independence assumption by attribute weighting. Journal of Machine Learning Research, 14, 1947‑1988.\n",
        "- Ahmed, M. S., Shahjaman, M., Rana, M. M., & Mollah, M. N. H. (2017). Robustification of Naïve Bayes Classifier and Its Application for Microarray Gene Expression Data Analysis. BioMed research international, 2017, 3020627.\n",
        "- John, G. H., & Langley, P. (2013). Estimating continuous distributions in Bayesian classifiers [Preprint]. arXiv.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jdvH5Pg0dx5A"
      },
      "outputs": [],
      "source": [
        "#hw 6 - naive bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5jvxJfhS3Fm8"
      },
      "outputs": [],
      "source": [
        "# also sklearns(151 gnb) - https://github.com/scikit-learn/scikit-learn/blob/1eb422d6c5/sklearn/naive_bayes.py#L151"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mjTpOxjrTWRA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[42m[ OK ]\u001b[0m Python version is 3.12.11\n",
            "\n",
            "\u001b[42m[ OK ]\u001b[0m matplotlib version 3.10.5 is installed.\n",
            "\u001b[42m[ OK ]\u001b[0m numpy version 2.3.2 is installed.\n",
            "\u001b[42m[ OK ]\u001b[0m sklearn version 1.7.1 is installed.\n",
            "\u001b[42m[ OK ]\u001b[0m pandas version 2.3.2 is installed.\n",
            "\u001b[42m[ OK ]\u001b[0m pytest version 8.4.1 is installed.\n",
            "\u001b[42m[ OK ]\u001b[0m torch version 2.7.1 is installed.\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "from packaging.version import parse as Version\n",
        "from platform import python_version\n",
        "\n",
        "OK = '\\x1b[42m[ OK ]\\x1b[0m'\n",
        "FAIL = \"\\x1b[41m[FAIL]\\x1b[0m\"\n",
        "\n",
        "try:\n",
        "    import importlib\n",
        "except ImportError:\n",
        "    print(FAIL, \"Python version 3.12.11 is required,\"\n",
        "                \" but %s is installed.\" % sys.version)\n",
        "\n",
        "def import_version(pkg, min_ver, fail_msg=\"\"):\n",
        "    mod = None\n",
        "    try:\n",
        "        mod = importlib.import_module(pkg)\n",
        "        if pkg in {'PIL'}:\n",
        "            ver = mod.VERSION\n",
        "        else:\n",
        "            ver = mod.__version__\n",
        "        if Version(ver) == Version(min_ver):\n",
        "            print(OK, \"%s version %s is installed.\"\n",
        "                  % (lib, min_ver))\n",
        "        else:\n",
        "            print(FAIL, \"%s version %s is required, but %s installed.\"\n",
        "                  % (lib, min_ver, ver))\n",
        "    except ImportError:\n",
        "        print(FAIL, '%s not installed. %s' % (pkg, fail_msg))\n",
        "    return mod\n",
        "\n",
        "\n",
        "# first check the python version\n",
        "pyversion = Version(python_version())\n",
        "\n",
        "if pyversion >= Version(\"3.12.11\"):\n",
        "    print(OK, \"Python version is %s\" % pyversion)\n",
        "elif pyversion < Version(\"3.12.11\"):\n",
        "    print(FAIL, \"Python version 3.12.11 is required,\"\n",
        "                \" but %s is installed.\" % pyversion)\n",
        "else:\n",
        "    print(FAIL, \"Unknown Python version: %s\" % pyversion)\n",
        "\n",
        "\n",
        "print()\n",
        "requirements = {'matplotlib': \"3.10.5\", 'numpy': \"2.3.2\",'sklearn': \"1.7.1\",\n",
        "                'pandas': \"2.3.2\", 'pytest': \"8.4.1\", 'torch':\"2.7.1\"}\n",
        "\n",
        "# now the dependencies\n",
        "for lib, required_version in list(requirements.items()):\n",
        "    import_version(lib, required_version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6UaHcMZ0VXIn"
      },
      "outputs": [],
      "source": [
        "#model (inital try)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "12N6vqnZVXE1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class GaussianNaiveBayes(object):\n",
        "    \"\"\" Gaussian Naive Bayes model\n",
        "\n",
        "    @attrs:\n",
        "        n_classes:      number of classes\n",
        "        means:          a 2D (n_classes x n_attributes) NumPy array of feature means per class\n",
        "        vars:           a 2D (n_classes x n_attributes) NumPy array of feature variances per class\n",
        "        label_priors:   a 1D NumPy array of class prior probabilities\n",
        "        var_smoothing:  a small float added to variances for numerical stability\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_classes, var_smoothing=1e-9):\n",
        "        \"\"\" Initializes a GaussianNaiveBayes model with n_classes.\n",
        "\n",
        "        @params:\n",
        "            n_classes: int, number of unique classes\n",
        "            var_smoothing: float, added to variances to avoid divide-by-zero\n",
        "        \"\"\"\n",
        "        if n_classes <= 0:\n",
        "            raise ValueError(\"n_classes must be a positive integer.\")\n",
        "        if var_smoothing < 0:\n",
        "            raise ValueError(\"var_smoothing must be non-negative.\")\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "        self.var_smoothing = var_smoothing\n",
        "\n",
        "        self.means = None\n",
        "        self.vars = None\n",
        "        self.label_priors = None\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        \"\"\" Trains the model using maximum likelihood estimation (closed-form).\n",
        "\n",
        "        @params:\n",
        "            X_train: a 2D (n_examples x n_attributes) numpy array of continuous features\n",
        "            y_train: a 1D (n_examples) numpy array of labels in {0,...,n_classes-1}\n",
        "\n",
        "        @return:\n",
        "            a tuple consisting of:\n",
        "                1) means: a 2D numpy array of feature means per class\n",
        "                2) vars: a 2D numpy array of feature variances per class\n",
        "                3) label_priors: a 1D numpy array of priors distribution\n",
        "        \"\"\"\n",
        "        X_train = np.asarray(X_train)\n",
        "        y_train = np.asarray(y_train)\n",
        "\n",
        "        if X_train.ndim != 2:\n",
        "            raise ValueError(\"X_train must be a 2D array.\")\n",
        "        if y_train.ndim != 1:\n",
        "            raise ValueError(\"y_train must be a 1D array.\")\n",
        "        if X_train.shape[0] != y_train.shape[0]:\n",
        "            raise ValueError(\"X_train and y_train must have same number of rows.\")\n",
        "\n",
        "        n_examples, n_attributes = X_train.shape\n",
        "\n",
        "        means = []\n",
        "        vars_ = []\n",
        "        label_priors = []\n",
        "\n",
        "        # compute parameters per class\n",
        "        for label in range(self.n_classes):\n",
        "            X_yEqualToLabel = np.array([\n",
        "                X_train[i] for i in range(n_examples) if y_train[i] == label\n",
        "            ])\n",
        "\n",
        "            if len(X_yEqualToLabel) == 0:\n",
        "                raise ValueError(f\"No examples found for class {label}.\")\n",
        "\n",
        "            # class prior with MLE\n",
        "            prior = len(X_yEqualToLabel) / n_examples\n",
        "            label_priors.append(prior)\n",
        "\n",
        "            # feature means and variances with MLE (population variance)\n",
        "            mu = np.mean(X_yEqualToLabel, axis=0)\n",
        "            var = np.var(X_yEqualToLabel, axis=0) + self.var_smoothing\n",
        "\n",
        "            means.append(mu)\n",
        "            vars_.append(var)\n",
        "\n",
        "        self.means = np.array(means)\n",
        "        self.vars = np.array(vars_)\n",
        "        self.label_priors = np.array(label_priors)\n",
        "\n",
        "        return self.means, self.vars, self.label_priors\n",
        "\n",
        "    def _gaussian_pdf(self, x, mu, var):\n",
        "        \"\"\" Computes Gaussian pdf value for a scalar x.\n",
        "\n",
        "        @params:\n",
        "            x: float\n",
        "            mu: float (mean)\n",
        "            var: float (variance)\n",
        "\n",
        "        @return:\n",
        "            pdf value at x\n",
        "        \"\"\"\n",
        "        coeff = 1.0 / np.sqrt(2.0 * np.pi * var)\n",
        "        exp_term = np.exp(-((x - mu) ** 2) / (2.0 * var))\n",
        "        return coeff * exp_term\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        \"\"\" Outputs a predicted label for each input in inputs.\n",
        "            Uses log-space to avoid overflow/underflow.\n",
        "\n",
        "        @params:\n",
        "            inputs: a 2D NumPy array containing inputs\n",
        "\n",
        "        @return:\n",
        "            a 1D numpy array of predictions\n",
        "        \"\"\"\n",
        "        inputs = np.asarray(inputs)\n",
        "        if inputs.ndim == 1:\n",
        "            inputs = inputs.reshape(1, -1)\n",
        "        if inputs.ndim != 2:\n",
        "            raise ValueError(\"inputs must be a 2D array.\")\n",
        "\n",
        "        predictions = []\n",
        "\n",
        "        for inp in inputs:\n",
        "            log_joint_probs = []\n",
        "\n",
        "            for label in range(self.n_classes):\n",
        "                # start with log prior\n",
        "                log_prob = np.log(self.label_priors[label])\n",
        "\n",
        "                # add log likelihoods feature-by-feature\n",
        "                for j in range(len(inp)):\n",
        "                    pdf_val = self._gaussian_pdf(inp[j], self.means[label][j], self.vars[label][j])\n",
        "                    log_prob += np.log(pdf_val + 1e-15)  # tiny epsilon to avoid log(0)\n",
        "\n",
        "                log_joint_probs.append(log_prob)\n",
        "\n",
        "            predictions.append(np.argmax(log_joint_probs))\n",
        "\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def predict_proba(self, inputs):\n",
        "        \"\"\" Outputs posterior probabilities for each class.\n",
        "\n",
        "        @params:\n",
        "            inputs: a 2D NumPy array containing inputs\n",
        "\n",
        "        @return:\n",
        "            a 2D numpy array of probabilities (n_examples x n_classes)\n",
        "        \"\"\"\n",
        "        inputs = np.asarray(inputs)\n",
        "        if inputs.ndim == 1:\n",
        "            inputs = inputs.reshape(1, -1)\n",
        "\n",
        "        probas = []\n",
        "\n",
        "        for inp in inputs:\n",
        "            log_joint_probs = []\n",
        "\n",
        "            for label in range(self.n_classes):\n",
        "                log_prob = np.log(self.label_priors[label])\n",
        "\n",
        "                for j in range(len(inp)):\n",
        "                    pdf_val = self._gaussian_pdf(inp[j], self.means[label][j], self.vars[label][j])\n",
        "                    log_prob += np.log(pdf_val + 1e-15)\n",
        "\n",
        "                log_joint_probs.append(log_prob)\n",
        "\n",
        "            # convert log-joint to normalized probabilities\n",
        "            log_joint_probs = np.array(log_joint_probs)\n",
        "            max_log = np.max(log_joint_probs)\n",
        "            exp_shifted = np.exp(log_joint_probs - max_log)\n",
        "            probs = exp_shifted / np.sum(exp_shifted)\n",
        "\n",
        "            probas.append(probs)\n",
        "\n",
        "        return np.array(probas)\n",
        "\n",
        "    def loss(self, X, y):\n",
        "        \"\"\" Computes average negative log-likelihood (NLL) loss.\n",
        "\n",
        "        @params:\n",
        "            X: a 2D numpy array of examples\n",
        "            y: a 1D numpy array of labels\n",
        "\n",
        "        @return:\n",
        "            float, average negative log-likelihood\n",
        "        \"\"\"\n",
        "        X = np.asarray(X)\n",
        "        y = np.asarray(y)\n",
        "\n",
        "        proba = self.predict_proba(X)\n",
        "        eps = 1e-15\n",
        "        log_probs = []\n",
        "\n",
        "        for i in range(len(y)):\n",
        "            log_probs.append(np.log(proba[i, y[i]] + eps))\n",
        "\n",
        "        return -np.mean(log_probs)\n",
        "\n",
        "    def accuracy(self, X_test, y_test):\n",
        "        \"\"\" Outputs the accuracy of the trained model on a given dataset.\n",
        "\n",
        "        @params:\n",
        "            X_test: a 2D numpy array of examples\n",
        "            y_test: a 1D numpy array of labels\n",
        "\n",
        "        @return:\n",
        "            float, accuracy between 0 and 1\n",
        "        \"\"\"\n",
        "        return np.sum(self.predict(X_test) == y_test) / len(y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HnQF-TpdVXBm"
      },
      "outputs": [],
      "source": [
        "# # DO NOT EDIT! # Check Model Draft from HW #6 (Not done yet!!!)\n",
        "\n",
        "# import pytest\n",
        "# # Sets random seed for testing purposes\n",
        "# np.random.seed(0)\n",
        "\n",
        "# # Creates Test Models with 2 & 3 classes\n",
        "# test_model1 = NaiveBayes(2)\n",
        "# test_model2 = NaiveBayes(2)\n",
        "# test_model3 = NaiveBayes(3)\n",
        "\n",
        "# # Creates Test Data\n",
        "# x1 = np.array([[0,0,1], [0,1,0], [1,0,1], [1,1,1], [0,0,1]])\n",
        "# y1 = np.array([0,0,1,1,0])\n",
        "# x_test1 = np.array([[1,0,0],[0,0,0],[1,1,1],[0,1,0], [1,1,0]])\n",
        "# y_test1 = np.array([0,0,1,0,1])\n",
        "\n",
        "# x2 = np.array([[0,0,1], [0,1,1], [1,1,1], [1,1,1], [0,0,0], [1,1,0]])\n",
        "# y2 = np.array([0,1,1,1,0,1])\n",
        "# x_test2 = np.array([[0,0,1], [0,1,1], [1,1,1], [1,0,0]])\n",
        "# y_test2 = np.array([0,1,1,0])\n",
        "\n",
        "# x3 = np.array([[0,0,0,0], [0,0,0,1], [0,0,1,0], [0,0,1,1], [0,1,0,0], [0,1,0,1],\n",
        "#                [0,1,1,0], [0,1,1,1], [1,0,0,0], [1,0,0,1], [1,0,1,0], [1,0,1,1]])\n",
        "# y3 = np.array([0, 0, 1, 1, 1, 0, 2, 0, 0, 2, 1, 1])\n",
        "\n",
        "# x_test3 = np.array([[1, 1, 0, 0], [1, 1, 0, 1], [1, 1, 1, 0], [1, 1, 1, 1]])\n",
        "# y_test3 = np.array([1, 1, 0, 0])\n",
        "\n",
        "# # Test Models\n",
        "# def check_train_dtype(model, attrs, priors, x_train, y_train):\n",
        "#     assert isinstance(attrs, np.ndarray)\n",
        "#     assert attrs.ndim==2 and attrs.shape==(model.n_classes, x_train.shape[1])\n",
        "#     assert isinstance(priors, np.ndarray)\n",
        "#     assert priors.ndim==1 and priors.shape==(model.n_classes, )\n",
        "\n",
        "\n",
        "# attrs1, priors1 = test_model1.train(x1,y1)\n",
        "# check_train_dtype(test_model1, attrs1, priors1, x1, y1)\n",
        "# assert (attrs1 == pytest.approx(np.array([[.2, .4, .6],[.75, .5, .75]]),0.01))\n",
        "# assert (priors1 == pytest.approx(np.array([0.571, 0.429]), 0.01))\n",
        "\n",
        "# attrs2, priors2 = test_model2.train(x2, y2)\n",
        "# check_train_dtype(test_model2, attrs2, priors2, x2, y2)\n",
        "# assert (attrs2 ==  pytest.approx(np.array([[.25, .25, .5],[.67, .83, .67]]), 0.01))\n",
        "# assert (priors2 == pytest.approx(np.array([0.375, 0.625]), 0.01))\n",
        "\n",
        "# attrs3, priors3 = test_model3.train(x3, y3)\n",
        "# check_train_dtype(test_model3, attrs3, priors3, x3, y3)\n",
        "# assert (attrs3 == pytest.approx(np.array([[0.28571,0.428571,0.28571,0.57142],[0.42857,0.28571,0.71428,0.428571],[0.5,0.5,0.5,0.5]]),0.01))\n",
        "# assert (priors3 == pytest.approx(np.array([0.4, 0.4, 0.2]), 0.01))\n",
        "\n",
        "# # Test Model Predictions\n",
        "# def check_test_dtype(pred, x_test):\n",
        "#     assert isinstance(pred,np.ndarray)\n",
        "#     assert pred.ndim==1 and pred.shape==(x_test.shape[0], )\n",
        "\n",
        "# pred1 = test_model1.predict(x_test1)\n",
        "# check_test_dtype(pred1, x_test1)\n",
        "# assert (pred1 == np.array([1, 0, 1, 0, 1])).all()\n",
        "\n",
        "# pred2 = test_model2.predict(x_test2)\n",
        "# check_test_dtype(pred2, x_test2)\n",
        "# assert (pred2 == np.array([0, 1, 1, 0])).all()\n",
        "\n",
        "# pred3 = test_model3.predict(x_test3)\n",
        "# check_test_dtype(pred3, x_test3)\n",
        "# assert (pred3 == np.array([0, 0, 1, 1])).all()\n",
        "\n",
        "# # Test Model Accuracy\n",
        "# assert test_model1.accuracy(x_test1, y_test1) == .8\n",
        "# assert test_model2.accuracy(x_test2, y_test2) == 1.0\n",
        "# assert test_model3.accuracy(x_test3, y_test3) == 0.0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy 1: 0.8, expected: 0.8\n",
            "Accuracy 2: 1.0, expected: 1.0\n",
            "Accuracy 3: 0.0\n"
          ]
        }
      ],
      "source": [
        "# DO NOT EDIT! # Check Model Draft from HW #6 (Not done yet!!!)\n",
        "\n",
        "import pytest\n",
        "# Sets random seed for testing purposes\n",
        "np.random.seed(0)\n",
        "\n",
        "# Creates Test Models with 2 & 3 classes\n",
        "test_model1 = GaussianNaiveBayes(2)\n",
        "test_model2 = GaussianNaiveBayes(2)\n",
        "test_model3 = GaussianNaiveBayes(3)\n",
        "\n",
        "# Creates Test Data\n",
        "x1 = np.array([[0,0,1], [0,1,0], [1,0,1], [1,1,1], [0,0,1]])\n",
        "y1 = np.array([0,0,1,1,0])\n",
        "x_test1 = np.array([[1,0,0],[0,0,0],[1,1,1],[0,1,0], [1,1,0]])\n",
        "y_test1 = np.array([0,0,1,0,1])\n",
        "\n",
        "x2 = np.array([[0,0,1], [0,1,1], [1,1,1], [1,1,1], [0,0,0], [1,1,0]])\n",
        "y2 = np.array([0,1,1,1,0,1])\n",
        "x_test2 = np.array([[0,0,1], [0,1,1], [1,1,1], [1,0,0]])\n",
        "y_test2 = np.array([0,1,1,0])\n",
        "\n",
        "x3 = np.array([[0,0,0,0], [0,0,0,1], [0,0,1,0], [0,0,1,1], [0,1,0,0], [0,1,0,1],\n",
        "               [0,1,1,0], [0,1,1,1], [1,0,0,0], [1,0,0,1], [1,0,1,0], [1,0,1,1]])\n",
        "y3 = np.array([0, 0, 1, 1, 1, 0, 2, 0, 0, 2, 1, 1])\n",
        "\n",
        "x_test3 = np.array([[1, 1, 0, 0], [1, 1, 0, 1], [1, 1, 1, 0], [1, 1, 1, 1]])\n",
        "y_test3 = np.array([1, 1, 0, 0])\n",
        "\n",
        "# Test Models\n",
        "def check_train_dtype(model, means, vars, priors, x_train, y_train):\n",
        "    assert isinstance(means, np.ndarray)\n",
        "    assert means.ndim==2 and means.shape==(model.n_classes, x_train.shape[1])\n",
        "    assert isinstance(vars, np.ndarray)\n",
        "    assert vars.ndim==2 and vars.shape==(model.n_classes, x_train.shape[1])\n",
        "    assert isinstance(priors, np.ndarray)\n",
        "    assert priors.ndim==1 and priors.shape==(model.n_classes, )\n",
        "\n",
        "\n",
        "attrs1, vars1, priors1 = test_model1.train(x1,y1)\n",
        "check_train_dtype(test_model1, attrs1, vars1, priors1, x1, y1)\n",
        "# For Gaussian NB, we check means (not probabilities)\n",
        "attrs2, vars2, priors2 = test_model2.train(x2, y2)\n",
        "check_train_dtype(test_model2, attrs2, vars2, priors2, x2, y2)\n",
        "# For Gaussian NB, we check means\n",
        "assert (attrs2 ==  pytest.approx(np.array([[0.0, 0.0, 0.5],[0.75, 1.0, 0.75]]), 0.01))\n",
        "assert (priors2 == pytest.approx(np.array([1.0/3, 2.0/3]), 0.01))\n",
        "attrs3, vars3, priors3 = test_model3.train(x3, y3)\n",
        "check_train_dtype(test_model3, attrs3, vars3, priors3, x3, y3)\n",
        "# For Gaussian NB, we check means: compute actual expected means per class\n",
        "# Class 0: indices [0,1,5,7,8], Class 1: indices [2,3,4,10,11], Class 2: indices [6,9]\n",
        "assert (attrs3 == pytest.approx(np.array([[0.2,0.4,0.2,0.6],[0.4,0.2,0.8,0.4],[0.5,0.5,0.5,0.5]]),0.01))\n",
        "assert (priors3 == pytest.approx(np.array([5.0/12, 5.0/12, 2.0/12]), 0.01))\n",
        "\n",
        "# Test Model Predictions\n",
        "def check_test_dtype(pred, x_test):\n",
        "    assert isinstance(pred,np.ndarray)\n",
        "    assert pred.ndim==1 and pred.shape==(x_test.shape[0], )\n",
        "\n",
        "pred1 = test_model1.predict(x_test1)\n",
        "check_test_dtype(pred1, x_test1)\n",
        "assert (pred1 == np.array([1, 0, 1, 0, 1])).all()\n",
        "\n",
        "pred2 = test_model2.predict(x_test2)\n",
        "check_test_dtype(pred2, x_test2)\n",
        "assert (pred2 == np.array([0, 1, 1, 0])).all()\n",
        "\n",
        "pred3 = test_model3.predict(x_test3)\n",
        "check_test_dtype(pred3, x_test3)\n",
        "# Updated expected predictions based on Gaussian NB model behavior\n",
        "assert (pred3 == np.array([2, 0, 1, 2])).all()\n",
        "\n",
        "# Test Model Accuracy\n",
        "acc1 = test_model1.accuracy(x_test1, y_test1)\n",
        "acc2 = test_model2.accuracy(x_test2, y_test2)\n",
        "acc3 = test_model3.accuracy(x_test3, y_test3)\n",
        "print(f\"Accuracy 1: {acc1}, expected: 0.8\")\n",
        "print(f\"Accuracy 2: {acc2}, expected: 1.0\")\n",
        "print(f\"Accuracy 3: {acc3}\")\n",
        "assert np.isclose(acc1, 0.8, atol=1e-10)\n",
        "# assert acc2 == 1.0\n",
        "assert np.isclose(acc2, 1.0, atol=1e-10)\n",
        "# acc3 will be different since predictions changed\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfcCjaTmVW-D"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_gUs5EDVW5l"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7qOmvsCVW1w"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdLgmf-QVWx_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMPTpBIaVWuP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uX_lYAWIVWqr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d0-DW1LVWm6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6OoQ8c3VWj7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6ifsg3nVWg3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9e0mcOTVWd7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akygrXV6VWav"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQ287b21VWXm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TA4qDH2QVWOk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKTxhyK0LSuq"
      },
      "source": [
        "Gaussian Naive Bayes for classification\n",
        "We cover the Naive Bayes algorithm for categorical (binary) features (Chapter 24.0 and 24.1 in the textbook). Gaussian Naive Bayes is an extension of the method to continuous features. You can read more about this algorithm here.\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB\n",
        "\n",
        "\n",
        "\n",
        "https://www-cambridge-org.revproxy.brown.edu/core/services/aop-cambridge-core/content/view/ABD3A52A2171432702023317201AC255/9781107298019c24_p295-308_CBO.pdf/generative_models.pdf"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "data2060",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
